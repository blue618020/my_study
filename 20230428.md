# 4월 28일 학습 내용
### 실습한 파일 목록
- [practice5](practice5.py)
- [crawling_study](crawling_study.py)

## Python
### 데이터 수집 단계
- 데이터 수집 --> 데이터 분석/처리 --> 인공지능 모델 학습 --> 인공지능 모델 평가 --> 사용
### http
- http (hypertext transfer protocol)
- request(요청) -- response(응답)
- client(클라이언트) -- server(서버)
- 클라이언트 : 서버에게 받는 쪽인 사용자 웹브라우저
- 클라이언트가 요청 --> 서버가 받고 응답을 돌려줌 
- 크롤링: 웹페이지로부터 데이터를 추출하는 행위를 말함

### html 
- html (hypertext markup language) 
- 웹 사이트를 표현하기 위한 언어
- 태그와 태그로 표현함 
- < html > < /html > 이거

### url 구조
- " 프로토콜://호스트주소 : 포트번호/경로 ? 쿼리=검색어 " 
- 쿼리이름=값&쿼리이름=값& 으로 반복되기도
- (예시) https://search.naver.com/search.naver?where=image&sm=tab_jum&query=%EC%B9%98%ED%82%A8
- 프로토콜 :// 호스트주소 / 경로 ? 어디인가=이미지  _& 쿼리=값(치킨으로 검색한거임)
----------------

### requests
- http 클라이언트 라이브러리
- 이걸 사용해서 쉽게 http 요청을 보낼 수 있음
- 터미널에 pip install requests 외부 라이브러리 설치
```py
import requests
url = "https://www.naver.com/"
response = requests.get(url)
```

#### http 상태코드
```py
status = response.status_code
print(status)  # 결과: 200
```
상태코드 | 의미
--------|------
200 | ok 요청 성공
302 | 리다이렉트 다른 페이지로 바로 연결
400 | Bad Request 요청이 잘못됨
401 | Unauthorizer 비인증됨
403 | Forvbidden 접근 권한 없음
404 | Not Found 요청받은 내용이 없음
405 | Method Nnot Allower 접근 방법이 잘못됨
500 | Internal Server Error 서버 에러
501 | Not Implemented
502 | Bad Gateway 잘못된 응답

### BeautifulSoup
- 뷰티풀수프
- requests를 이용해 가져온 텍스트 데이터에서 원하는 html 태그를 추출하게 하는 라이브러리. 둘이 같이 사용함
- html 파싱 (parsing) 또는 html을 객체구조화해서 사용한다 라고 함
- 터미널에 pip install beautifulsoup4 외부 라이브러리 설치
```py
from bs4 import BeautifulSoup
html = "<html><body>hello</body></html>"  # (requests를 사용해서 가져온 대충 html 태그들)
soup = BeautifulSoup(html, "html.parser")
```

-------------

### 네이버에서 IT/과학 뉴스 크롤링하기 (실습)
#### https:// 링크 가져오기
```py
import requests
from bs4 import BeautifulSoup
url = "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105" 
response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}) 
```
- requests와 BeautifulSoup 라이브러리를 선언
- url = 네이버 IT/과학 뉴스 링크를 가져온 것
- requests . get( url )로, html 내용들에게 접근 (크롤링)
- (참고: headers={"User-Agent": "Mozilla/5.0"} --> 네이버가 크롤링을 막아놔서 봇 아닌척 넣은 코드로, 크롤링 방지 회피(이걸뚫네))

#### 뉴스 제목들 추출하기
```py
html = response.text  
soup = BeautifulSoup(html, "html.parser")
div = soup.body.find('div', attrs={'class': 'list_body'}) # '헤드라인 뉴스'쪽만 가져온 것
headlines = div.find_all('a', attrs={'class': 'cluster_text_headline'})  # 뉴스의 제목이 있는 a 태그만 가져온 것
```
- html 변수에 텍스트들 긁어온거 넣음
- 원하는 텍스트를 가져오기 위해 BeautifulSoup 사용
- 파이썬에서 html 테그를 객체 멤버 접근 연산자( . )로 표현해서 접근 할 수 있음 
